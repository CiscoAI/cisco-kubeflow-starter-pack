{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Hybrid BLE-RSSI Pipeline Notebook\n",
    "\n",
    "This Notebook allows us to train the model on UCS and deploy the model on any cloud of choice ( AWS or GCP or Azure) from one point, provided required parameters and secrets for the chosen cloud are set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Cisco Kubeflow Starter Pack repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_NAME=\"hybrid\" #Provide git branch as \"master\"/\"dev\"/\"hybrid\"\n",
    "! git clone -b $BRANCH_NAME https://github.com/CiscoAI/cisco-kubeflow-starter-pack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kfp pandas numpy --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install specific packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages below if using aws cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install boto3 sagemaker mxnet --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages below if using azure cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azureml-core --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set name of cloud to be used for model deployment ( aws or gcp or azure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD_NAME=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CLOUD_NAME or CLOUD_NAME not in ('aws','gcp','azure'):\n",
    "     raise ValueError(\"Set name of the cloud on which you need to deploy your model: gcp/aws/azure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "import kfp.dsl as dsl\n",
    "from kubernetes import client as k8s_client\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Parameters file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file_path='cisco-kubeflow-starter-pack/apps/networking/ble-localization/hybrid/integrated/'\n",
    "sys.path.insert(1, params_file_path)\n",
    "import parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "        \n",
    "        from kfp.aws import use_aws_secret\n",
    "        import boto3\n",
    "        import pandas as pd\n",
    "        import sagemaker\n",
    "        import mxnet as mx\n",
    "        from mxnet import nd\n",
    "        from sagemaker.mxnet import MXNetModel\n",
    "        from sagemaker.predictor import json_serializer, json_deserializer, RealTimePredictor\n",
    "        \n",
    "elif CLOUD_NAME=='gcp':\n",
    "\n",
    "        import kfp.gcp as gcp\n",
    "        import kfp.dsl as dsl\n",
    "        import googleapiclient.discovery\n",
    "        \n",
    "elif CLOUD_NAME=='azure':\n",
    "\n",
    "        import base64\n",
    "        from azureml.core.webservice import AciWebservice\n",
    "        from azureml.core.webservice import Webservice\n",
    "        from azureml.core.model import Model\n",
    "        from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "        from azureml.core import Workspace\n",
    "        from azureml.core.image import Image\n",
    "        from azureml.core import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import parameters and set to local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "        from parameters import Aws_params\n",
    "        execution_mode, bucket_name, secret_name = Aws_params.execution_mode, Aws_params.bucket_name, Aws_params.secret_name \n",
    "        aws_cloud_region, model_name, instance_type, role_arn = Aws_params.aws_cloud_region, Aws_params.model_name, Aws_params.instance_type, Aws_params.role_arn \n",
    "        inference_image, endpoint_config_name, endpoint_name, model_path = Aws_params.inference_image, Aws_params.endpoint_config_name, Aws_params.endpoint_name, Aws_params.model_path\n",
    "        \n",
    "        %env AWS_DEFAULT_REGION={aws_cloud_region}\n",
    "        \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "        from parameters import Gcp_params\n",
    "        execution_mode, bucket_name, secret_name, gcp_cloud_region = Gcp_params.execution_mode, Gcp_params.bucket_name, Gcp_params.secret_name, Gcp_params.gcp_cloud_region \n",
    "        model_name, instance_type, google_application_credentials = Gcp_params.model_name, Gcp_params.instance_type, Gcp_params.google_application_credentials\n",
    "        version_name, model_path, project_id = Gcp_params.version_name, Gcp_params.model_path, Gcp_params.project_id\n",
    "\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS={google_application_credentials}\n",
    "        \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "       from parameters import Azure_params\n",
    "       azure_model, azure_service = Azure_params.azure_model, Azure_params.azure_service       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws' or CLOUD_NAME=='gcp':\n",
    "    \n",
    "        if execution_mode == \"local\" and host == '':\n",
    "                 raise ValueError(\"Please set host to the appropriate URL\")\n",
    "        elif execution_mode != \"local\":\n",
    "                 execution_mode = \"in-cluster\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    awsParams = [execution_mode, bucket_name, secret_name, instance_type, aws_cloud_region, model_name, role_arn, inference_image, endpoint_config_name, endpoint_name, model_path]\n",
    "    for param in awsParams:\n",
    "        if not param:\n",
    "            raise ValueError(\"One of the parameters in the aws_params list is missing. Please check whether all parameters are entered values\")\n",
    "            \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    gcpParams = [execution_mode, bucket_name, secret_name, instance_type, google_application_credentials, gcp_cloud_region, model_name, version_name, model_path, project_id]\n",
    "    for param in gcpParams:\n",
    "        if not param:\n",
    "            raise ValueError(\"One of the parameters in the gcp_params list is missing. Please check whether all parameters are entered values\")\n",
    "            \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    azureParams = [azure_model, azure_service]\n",
    "    for param in azureParams:\n",
    "        if not param:\n",
    "            raise ValueError(\"One of the parameters in the azure_params list is missing. Please check whether all parameters are entered values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load components & declare environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    model = 'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/aws/sagemaker/model/component.yaml'\n",
    "    deploy = 'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/aws/sagemaker/deploy/component.yaml'\n",
    "    \n",
    "    sagemaker_model_op = components.load_component_from_url(model)\n",
    "    sagemaker_deploy_op = components.load_component_from_url(deploy)\n",
    "\n",
    "    def blerssi_aws_train_upload_op(step_name='aws-train'):\n",
    "        return dsl.ContainerOp(\n",
    "            name='aws-train-upload-s3',\n",
    "            image='ciscoai/mxnet-blerssi-train-upload:v0.2',\n",
    "            command=['python', '/opt/mx-dnn.py', 'train'],\n",
    "            arguments=['--bucket-name', bucket_name]\n",
    "        ).apply(use_aws_secret(secret_name=secret_name, aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    from parameters import Timestamp\n",
    "    deploy=\"https://raw.githubusercontent.com/kubeflow/pipelines/01a23ae8672d3b18e88adf3036071496aca3552d/components/gcp/ml_engine/deploy/component.yaml\"\n",
    "    \n",
    "    mlengine_deploy_op = components.load_component_from_url(deploy)\n",
    "\n",
    "    def blerssi_train_upload_op(step_name='blerssi-train'):\n",
    "        return dsl.ContainerOp(\n",
    "            name='blerssi-train-upload-gcp',\n",
    "            image='docker.io/samba07/blerssi-gcp-mlengine:0.2',\n",
    "            command=['python', '/opt/blerssi-model.py'],\n",
    "            arguments=['--bucket-name', bucket_name,\n",
    "                       '--model-version', Timestamp.gcp_timestamp]\n",
    "        ).apply(gcp.use_gcp_secret(secret_name))\n",
    "\n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    path='cisco-kubeflow-starter-pack/apps/networking/ble-localization/hybrid/azure/pipelines/'\n",
    "    component_root_train = path + 'components/train_model/'\n",
    "    component_root_register = path + 'components/register_model/'\n",
    "    component_root_deploy = path + 'components/deploy_model/'\n",
    "    \n",
    "    azure_train_op = kfp.components.load_component_from_file(os.path.join(component_root_train, 'component.yaml'))\n",
    "    azure_register_op = kfp.components.load_component_from_file(os.path.join(component_root_register, 'component.yaml'))\n",
    "    azure_deploy_op = kfp.components.load_component_from_file(os.path.join(component_root_deploy, 'component.yaml'))\n",
    "    \n",
    "    nfs_pvc = k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs')\n",
    "    nfs_volume = k8s_client.V1Volume(name='nfs', persistent_volume_claim=nfs_pvc)\n",
    "    nfs_volume_mount = k8s_client.V1VolumeMount(mount_path='/mnt/', name='nfs')\n",
    "    \n",
    "    workspace_name = os.getenv('WORKSPACE_NAME')\n",
    "    subscription_id = os.getenv('SUBSCRIPTION_ID')\n",
    "    resource_group = os.getenv('RESOURCE_GROUP')\n",
    "    tenant_id = os.getenv('TENANT_ID')\n",
    "    service_principal_id = os.getenv('SERVICE_PRINCIPAL_ID')\n",
    "    service_principal_password = os.getenv('SERVICE_PRINCIPAL_PASSWORD')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    @dsl.pipeline(\n",
    "    name='AWS Sagemaker Hybrid Pipeline',\n",
    "    description='Pipeline to train BLERSSI model using mxnet and save in aws s3 bucket'\n",
    "    )\n",
    "    def aws_pipeline(\n",
    "        region=\"\",\n",
    "        image=\"\",\n",
    "        model_name=\"\",\n",
    "        endpoint_config_name=\"\",\n",
    "        endpoint_name=\"\",\n",
    "        model_artifact_url=\"\",\n",
    "        instance_type_1=\"\",\n",
    "        role=\"\"\n",
    "    ):\n",
    "        train_upload_model = blerssi_aws_train_upload_op()\n",
    "\n",
    "        create_model = sagemaker_model_op(\n",
    "            region=region,\n",
    "            model_name=model_name,\n",
    "            image=image,\n",
    "            model_artifact_url=model_artifact_url,\n",
    "            role=role\n",
    "        ).apply(use_aws_secret(secret_name=secret_name, aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "        create_model.after(train_upload_model)\n",
    "\n",
    "        sagemaker_deploy=sagemaker_deploy_op(\n",
    "            region=region,\n",
    "            endpoint_config_name=endpoint_config_name,\n",
    "            endpoint_name=endpoint_name,\n",
    "            model_name_1=create_model.output,\n",
    "            instance_type_1=instance_type_1\n",
    "        ).apply(use_aws_secret(secret_name=secret_name, aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "        sagemaker_deploy.after(create_model)\n",
    "        \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    @dsl.pipeline(\n",
    "    name='CloudML deploy pipeline',\n",
    "    description='CloudML deploy pipeline'\n",
    "     )\n",
    "    def gcp_pipeline(\n",
    "          model_uri = '',\n",
    "          project_id = '',\n",
    "          model_id = '',\n",
    "          version_id = '',\n",
    "          runtime_version = '1.10',\n",
    "          python_version = '',\n",
    "          version = '',\n",
    "          replace_existing_version = 'False',\n",
    "          set_default = 'True',\n",
    "          wait_interval = '30'):\n",
    "\n",
    "        train_upload_model = blerssi_train_upload_op()\n",
    "\n",
    "        task = mlengine_deploy_op(\n",
    "            model_uri=model_uri, \n",
    "            project_id=project_id, \n",
    "            model_id=model_id, \n",
    "            version_id=version_id, \n",
    "            runtime_version=runtime_version, \n",
    "            python_version=python_version,\n",
    "            version=version, \n",
    "            replace_existing_version=replace_existing_version, \n",
    "            set_default=set_default, \n",
    "            wait_interval=wait_interval).apply(gcp.use_gcp_secret(secret_name))\n",
    "        task.after(train_upload_model)\n",
    "        \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    def azure_pipeline():\n",
    "    \n",
    "            #Define task for training BLERSSI data\n",
    "            azure_train_task = azure_train_op()\n",
    "            azure_train_task.add_volume(nfs_volume)\n",
    "            azure_train_task.add_volume_mount(nfs_volume_mount)\n",
    "\n",
    "            #Define task for registering BLERSSI model on Azure\n",
    "            azure_register_task = azure_register_op(workspace_name=workspace_name,\n",
    "                                                   subscription_id=subscription_id,\n",
    "                                                   resource_group=resource_group,\n",
    "                                                   model_name=azure_model,\n",
    "                                                   tenant_id=tenant_id,\n",
    "                                                   service_principal_id=service_principal_id,\n",
    "                                                   service_principal_password=service_principal_password)\n",
    "\n",
    "            azure_register_task.add_volume(nfs_volume)\n",
    "            azure_register_task.add_volume_mount(nfs_volume_mount)\n",
    "            azure_register_task.after(azure_train_task)\n",
    "\n",
    "            #Define Task for deploying BLERSSI model on Azure \n",
    "            azure_deploy_task = azure_deploy_op(workspace_name=workspace_name,\n",
    "                                                subscription_id=subscription_id,\n",
    "                                                resource_group=resource_group,\n",
    "                                                model_name=azure_model,\n",
    "                                                service_name=azure_service,\n",
    "                                                tenant_id=tenant_id,\n",
    "                                                service_principal_id=service_principal_id,\n",
    "                                                service_principal_password=service_principal_password)\n",
    "\n",
    "            azure_deploy_task.after(azure_register_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    from parameters import Timestamp\n",
    "    try:\n",
    "        import kfp.compiler as compiler\n",
    "        compiler.Compiler().compile(aws_pipeline, 'blerssi_aws_pipeline.tar.gz')\n",
    "    except RuntimeError as err:\n",
    "        logging.debug(err)\n",
    "        logging.info(\"Argo workflow failed validation check but it can still be used to run experiments.\")\n",
    "        \n",
    "    client = None\n",
    "    if execution_mode == \"local\":\n",
    "        client = kfp.Client(host=host)\n",
    "    else:\n",
    "        client = kfp.Client()\n",
    "    blerssi_hybrid_experiment = client.create_experiment(name='BLERSSI-Sagemaker')\n",
    "\n",
    "    run = client.run_pipeline(blerssi_hybrid_experiment.id, 'blerssi-sagemaker-pipeline-'+Timestamp.timestamp, pipeline_package_path='blerssi_aws_pipeline.tar.gz', params={\n",
    "'region': aws_cloud_region,\n",
    "'image': inference_image,\n",
    "'model_name': model_name,\n",
    "'endpoint_config_name': endpoint_config_name,\n",
    "'endpoint_name': endpoint_name,\n",
    "'model_artifact_url': model_path,\n",
    "'instance_type_1': instance_type,\n",
    "'role': role_arn\n",
    "})\n",
    "\n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    from parameters import Timestamp\n",
    "    try:\n",
    "        import kfp.compiler as compiler\n",
    "        compiler.Compiler().compile(gcp_pipeline, 'blerssi_gcp_pipeline.tar.gz')\n",
    "    except RuntimeError as err:\n",
    "        logging.debug(err)\n",
    "        logging.info(\"Argo workflow failed validation check but it can still be used to run experiments.\")\n",
    "        \n",
    "    client = None\n",
    "    if execution_mode == \"local\":\n",
    "        client = kfp.Client(host=host)\n",
    "    else:\n",
    "        client = kfp.Client()\n",
    "    blerssi_hybrid_experiment = client.create_experiment(name='BLERSSI-GCP')\n",
    "        \n",
    "    run = client.run_pipeline(blerssi_hybrid_experiment.id, 'blerssi-gcp-mlengine-pipeline-'+Timestamp.gcp_timestamp, pipeline_package_path='blerssi_gcp_pipeline.tar.gz',\n",
    "                         params={'model_uri': model_path,\n",
    "                                 'project_id': project_id,\n",
    "                                 'model_id': model_name,\n",
    "                                 'version_id': version_name})\n",
    "    \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    #Create a pipeline run\n",
    "    kfp.Client().create_run_from_pipeline_func(azure_pipeline, arguments={})\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check service endpoint status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    sg_client = boto3.client('sagemaker', region_name=aws_cloud_region)\n",
    "    resp = sg_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = resp['EndpointStatus']\n",
    "    print(\"Endpoint status:\", endpoint_status)\n",
    "    logging.info(f\"Endpoint status: {endpoint_status}\")\n",
    "    \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}/versions/{}'.format(project_id, model_name, version_name)\n",
    "    \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    svc_pr_password = os.environ.get(\"AZUREML_PASSWORD\")\n",
    "    svc_pr = ServicePrincipalAuthentication(\n",
    "    tenant_id=tenant_id,\n",
    "    service_principal_id=service_principal_id,\n",
    "    service_principal_password=service_principal_password)\n",
    "\n",
    "    ws = Workspace(\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group=resource_group,\n",
    "    workspace_name=workspace_name,\n",
    "    auth=svc_pr\n",
    "    )\n",
    "    \n",
    "    service = Webservice(workspace=ws, name=azure_service)\n",
    "    print(service.scoring_uri)\n",
    "    \n",
    "    print(service.get_logs())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using service endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, content_type= 'application/x-npy', accept= 'application/json')\n",
    "\n",
    "    def _npy_dumps(data):\n",
    "        \"\"\"\n",
    "        Serialized a numpy array into a stream of npy-formatted bytes.\n",
    "        \"\"\"\n",
    "        from six import BytesIO\n",
    "        buffer = BytesIO()\n",
    "        np.save(buffer, data)\n",
    "        return buffer.getvalue()\n",
    "\n",
    "    request_data = _npy_dumps(nd.array([[-200, -200, -200, -75, -200, -200, -200, -200, -200, -200, -200, -200, -200],[-200, -200, -200, -75, -200, -200, -200, -200, -200, -200, -200, -200, -200]]).asnumpy())\n",
    "    result = predictor.predict(data=request_data)\n",
    "\n",
    "    import pickle\n",
    "    depickled_result = pickle.loads(result)\n",
    "\n",
    "    print(\"Outputs, predictions\")\n",
    "    print(depickled_result[0], depickled_result[1])\n",
    "    \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    instances=[{\"b3001\":[1.0] , \"b3002\":[1.0] , \"b3003\":[0.4], \"b3004\":[1.0] , \"b3005\":[0.385] , \"b3006\":[0.280] , \"b3007\":[0.405] , \"b3008\":[1.0] , \"b3009\":[1.0] , \"b3010\":[1.0] , \"b3011\":[1.0] , \"b3012\":[1.0] , \"b3013\":[1.0]}]\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "    else:\n",
    "      print(response['predictions'])\n",
    "\n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    X=[[-200,-200,-200,-200,-200,-63,-200,-200,-200,-200,-200,-200,-200]]\n",
    "    test_samples = json.dumps({\"data\": X})\n",
    "    test_samples = bytes(test_samples, encoding='utf8')\n",
    "    \n",
    "    print(service.run(input_data=test_samples))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up after prediction\n",
    "\n",
    "In case of GCP cloud, please enter service account name & project ID in the placeholders below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    logging.info(\"Deleting endpoint...\")\n",
    "    predictor.delete_endpoint()\n",
    "    \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    !gcloud auth activate-service-account <<ACCOUNT>> --key-file=auth.json --project=<<PROJECT ID>>\n",
    "\n",
    "    # Delete version resource\n",
    "    ! gcloud ai-platform versions delete $version_name --quiet --model $model_name \n",
    "\n",
    "    # Delete model resource\n",
    "    ! gcloud ai-platform models delete $model_name --quiet\n",
    "\n",
    "    # Delete Cloud Storage objects that were created\n",
    "    ! gsutil -m rm -r gs://$bucket_name/\n",
    "        \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    #Delete/cleanup created webservice\n",
    "    service.delete()\n",
    "\n",
    "    #Delete created image \n",
    "    image_obj = Image(ws, name=azure_service)\n",
    "    image_obj.delete()\n",
    "\n",
    "    #Delete created model\n",
    "    model_obj = Model(ws, name=azure_model)\n",
    "    model_obj.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
