{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Hybrid BLE-RSSI Pipeline Notebook\n",
    "\n",
    "This Notebook allows us to train the model on UCS and deploy the model on any cloud of choice ( AWS or GCP or Azure) from one point, provided required parameters and secrets for the chosen cloud are set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Cisco Kubeflow Starter Pack repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_NAME=\"hybrid\" #Provide git branch as \"master\"/\"dev\"/\"hybrid\"\n",
    "! git clone -b $BRANCH_NAME https://github.com/CiscoAI/cisco-kubeflow-starter-pack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in ./.local/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.6/site-packages (1.0.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.1)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp) (0.1.9)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (10.0.1)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp) (5.3)\n",
      "Requirement already satisfied: kfp-server-api<0.6.0,>=0.2.5 in ./.local/lib/python3.6/site-packages (from kfp) (0.5.0)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp) (7.1.2)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.25.0)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (1.11.0)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp) (1.2.10)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp) (0.30.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.11.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2019.11.28)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (45.1.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.25.8)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp) (0.5.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (0.15.7)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp) (1.4.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp) (1.11.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp) (2.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.51.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.11.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kfp pandas numpy --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install specific packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages below if using aws cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install boto3 sagemaker mxnet --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages below if using azure cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azureml-core --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set name of cloud to be used for model deployment ( aws or gcp or azure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD_NAME=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CLOUD_NAME or CLOUD_NAME not in ('aws','gcp','azure'):\n",
    "     raise ValueError(\"Set name of the cloud on which you need to deploy your model: gcp/aws/azure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "import kfp.dsl as dsl\n",
    "from kubernetes import client as k8s_client\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Parameters file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file_path='cisco-kubeflow-starter-pack/apps/networking/ble-localization/hybrid/integrated/'\n",
    "sys.path.insert(1, params_file_path)\n",
    "import parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import specific libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "        \n",
    "        from kfp.aws import use_aws_secret\n",
    "        import boto3\n",
    "        import pandas as pd\n",
    "        import sagemaker\n",
    "        import mxnet as mx\n",
    "        from mxnet import nd\n",
    "        from sagemaker.mxnet import MXNetModel\n",
    "        from sagemaker.predictor import json_serializer, json_deserializer, RealTimePredictor\n",
    "        \n",
    "elif CLOUD_NAME=='gcp':\n",
    "\n",
    "        import kfp.gcp as gcp\n",
    "        import kfp.dsl as dsl\n",
    "        import googleapiclient.discovery\n",
    "        \n",
    "elif CLOUD_NAME=='azure':\n",
    "\n",
    "        import base64\n",
    "        from azureml.core.webservice import AciWebservice\n",
    "        from azureml.core.webservice import Webservice\n",
    "        from azureml.core.model import Model\n",
    "        from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "        from azureml.core import Workspace\n",
    "        from azureml.core.image import Image\n",
    "        from azureml.core import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import parameters and set to local variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "        from parameters import Aws_params\n",
    "        execution_mode, bucket_name, secret_name = Aws_params.execution_mode, Aws_params.bucket_name, Aws_params.secret_name \n",
    "        aws_cloud_region, model_name, instance_type, role_arn = Aws_params.aws_cloud_region, Aws_params.model_name, Aws_params.instance_type, Aws_params.role_arn \n",
    "        inference_image, endpoint_config_name, endpoint_name, model_path = Aws_params.inference_image, Aws_params.endpoint_config_name, Aws_params.endpoint_name, Aws_params.model_path\n",
    "        \n",
    "        %env AWS_DEFAULT_REGION={aws_cloud_region}\n",
    "        \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "        from parameters import Gcp_params\n",
    "        execution_mode, bucket_name, secret_name, gcp_cloud_region = Gcp_params.execution_mode, Gcp_params.bucket_name, Gcp_params.secret_name, Gcp_params.gcp_cloud_region \n",
    "        model_name, instance_type, google_application_credentials = Gcp_params.model_name, Gcp_params.instance_type, Gcp_params.google_application_credentials\n",
    "        version_name, model_path, project_id = Gcp_params.version_name, Gcp_params.model_path, Gcp_params.project_id\n",
    "\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS={google_application_credentials}\n",
    "        \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "       from parameters import Azure_params\n",
    "       azure_model, azure_service = Azure_params.azure_model, Azure_params.azure_service       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws' or CLOUD_NAME=='gcp':\n",
    "    \n",
    "        if execution_mode == \"local\" and host == '':\n",
    "                 raise ValueError(\"Please set host to the appropriate URL\")\n",
    "        elif execution_mode != \"local\":\n",
    "                 execution_mode = \"in-cluster\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    awsParams = [execution_mode, bucket_name, secret_name, instance_type, aws_cloud_region, model_name, role_arn, inference_image, endpoint_config_name, endpoint_name, model_path]\n",
    "    for param in awsParams:\n",
    "        if not param:\n",
    "            raise ValueError(\"One of the parameters in the aws_params list is missing. Please check whether all parameters are entered values\")\n",
    "            \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    gcpParams = [execution_mode, bucket_name, secret_name, instance_type, google_application_credentials, gcp_cloud_region, model_name, version_name, model_path, project_id]\n",
    "    for param in gcpParams:\n",
    "        if not param:\n",
    "            raise ValueError(\"One of the parameters in the gcp_params list is missing. Please check whether all parameters are entered values\")\n",
    "            \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    azureParams = [azure_model, azure_service]\n",
    "    for param in azureParams:\n",
    "        if not param:\n",
    "            raise ValueError(\"One of the parameters in the azure_params list is missing. Please check whether all parameters are entered values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load components & declare environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    model = 'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/aws/sagemaker/model/component.yaml'\n",
    "    deploy = 'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/aws/sagemaker/deploy/component.yaml'\n",
    "    \n",
    "    sagemaker_model_op = components.load_component_from_url(model)\n",
    "    sagemaker_deploy_op = components.load_component_from_url(deploy)\n",
    "\n",
    "    def blerssi_mxnet_train_upload_op(step_name='mxnet-train'):\n",
    "        return dsl.ContainerOp(\n",
    "            name='mxnet-train-upload-s3',\n",
    "            image='ciscoai/mxnet-blerssi-train-upload:v0.2',\n",
    "            command=['python', '/opt/mx-dnn.py', 'train'],\n",
    "            arguments=['--bucket-name', bucket_name]\n",
    "        ).apply(use_aws_secret(secret_name=secret_name, aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    from parameters import Timestamp\n",
    "    deploy=\"https://raw.githubusercontent.com/kubeflow/pipelines/01a23ae8672d3b18e88adf3036071496aca3552d/components/gcp/ml_engine/deploy/component.yaml\"\n",
    "    \n",
    "    mlengine_deploy_op = components.load_component_from_url(deploy)\n",
    "\n",
    "    def blerssi_train_upload_op(step_name='blerssi-train'):\n",
    "        return dsl.ContainerOp(\n",
    "            name='blerssi-train-upload-gcp',\n",
    "            image='docker.io/samba07/blerssi-gcp-mlengine:0.2',\n",
    "            command=['python', '/opt/blerssi-model.py'],\n",
    "            arguments=['--bucket-name', bucket_name,\n",
    "                       '--model-version', Timestamp.gcp_timestamp]\n",
    "        ).apply(gcp.use_gcp_secret(secret_name))\n",
    "\n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    path='cisco-kubeflow-starter-pack/apps/networking/ble-localization/hybrid/azure/pipelines/'\n",
    "    component_root_train = path + 'components/train_model/'\n",
    "    component_root_register = path + 'components/register_model/'\n",
    "    component_root_deploy = path + 'components/deploy_model/'\n",
    "    \n",
    "    azure_train_op = kfp.components.load_component_from_file(os.path.join(component_root_train, 'component.yaml'))\n",
    "    azure_register_op = kfp.components.load_component_from_file(os.path.join(component_root_register, 'component.yaml'))\n",
    "    azure_deploy_op = kfp.components.load_component_from_file(os.path.join(component_root_deploy, 'component.yaml'))\n",
    "    \n",
    "    nfs_pvc = k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs')\n",
    "    nfs_volume = k8s_client.V1Volume(name='nfs', persistent_volume_claim=nfs_pvc)\n",
    "    nfs_volume_mount = k8s_client.V1VolumeMount(mount_path='/mnt/', name='nfs')\n",
    "    \n",
    "    workspace_name = os.getenv('WORKSPACE_NAME')\n",
    "    subscription_id = os.getenv('SUBSCRIPTION_ID')\n",
    "    resource_group = os.getenv('RESOURCE_GROUP')\n",
    "    tenant_id = os.getenv('TENANT_ID')\n",
    "    service_principal_id = os.getenv('SERVICE_PRINCIPAL_ID')\n",
    "    service_principal_password = os.getenv('SERVICE_PRINCIPAL_PASSWORD')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    @dsl.pipeline(\n",
    "    name='MXNet Sagemaker Hybrid Pipeline',\n",
    "    description='Pipeline to train BLERSSI model using mxnet and save in aws s3 bucket'\n",
    "    )\n",
    "    def mxnet_pipeline(\n",
    "        region=\"\",\n",
    "        image=\"\",\n",
    "        model_name=\"\",\n",
    "        endpoint_config_name=\"\",\n",
    "        endpoint_name=\"\",\n",
    "        model_artifact_url=\"\",\n",
    "        instance_type_1=\"\",\n",
    "        role=\"\"\n",
    "    ):\n",
    "        train_upload_model = blerssi_mxnet_train_upload_op()\n",
    "\n",
    "        create_model = sagemaker_model_op(\n",
    "            region=region,\n",
    "            model_name=model_name,\n",
    "            image=image,\n",
    "            model_artifact_url=model_artifact_url,\n",
    "            role=role\n",
    "        ).apply(use_aws_secret(secret_name=secret_name, aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "        create_model.after(train_upload_model)\n",
    "\n",
    "        sagemaker_deploy=sagemaker_deploy_op(\n",
    "            region=region,\n",
    "            endpoint_config_name=endpoint_config_name,\n",
    "            endpoint_name=endpoint_name,\n",
    "            model_name_1=create_model.output,\n",
    "            instance_type_1=instance_type_1\n",
    "        ).apply(use_aws_secret(secret_name=secret_name, aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "        sagemaker_deploy.after(create_model)\n",
    "        \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    @dsl.pipeline(\n",
    "    name='CloudML deploy pipeline',\n",
    "    description='CloudML deploy pipeline'\n",
    "     )\n",
    "    def gcp_pipeline(\n",
    "          model_uri = '',\n",
    "          project_id = '',\n",
    "          model_id = '',\n",
    "          version_id = '',\n",
    "          runtime_version = '1.10',\n",
    "          python_version = '',\n",
    "          version = '',\n",
    "          replace_existing_version = 'False',\n",
    "          set_default = 'True',\n",
    "          wait_interval = '30'):\n",
    "\n",
    "        train_upload_model = blerssi_train_upload_op()\n",
    "\n",
    "        task = mlengine_deploy_op(\n",
    "            model_uri=model_uri, \n",
    "            project_id=project_id, \n",
    "            model_id=model_id, \n",
    "            version_id=version_id, \n",
    "            runtime_version=runtime_version, \n",
    "            python_version=python_version,\n",
    "            version=version, \n",
    "            replace_existing_version=replace_existing_version, \n",
    "            set_default=set_default, \n",
    "            wait_interval=wait_interval).apply(gcp.use_gcp_secret(secret_name))\n",
    "        task.after(train_upload_model)\n",
    "        \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    def azure_pipeline():\n",
    "    \n",
    "            #Define task for training BLERSSI data\n",
    "            azure_train_task = azure_train_op()\n",
    "            azure_train_task.add_volume(nfs_volume)\n",
    "            azure_train_task.add_volume_mount(nfs_volume_mount)\n",
    "\n",
    "            #Define task for registering BLERSSI model on Azure\n",
    "            azure_register_task = azure_register_op(workspace_name=workspace_name,\n",
    "                                                   subscription_id=subscription_id,\n",
    "                                                   resource_group=resource_group,\n",
    "                                                   model_name=azure_model,\n",
    "                                                   tenant_id=tenant_id,\n",
    "                                                   service_principal_id=service_principal_id,\n",
    "                                                   service_principal_password=service_principal_password)\n",
    "\n",
    "            azure_register_task.add_volume(nfs_volume)\n",
    "            azure_register_task.add_volume_mount(nfs_volume_mount)\n",
    "            azure_register_task.after(azure_train_task)\n",
    "\n",
    "            #Define Task for deploying BLERSSI model on Azure \n",
    "            azure_deploy_task = azure_deploy_op(workspace_name=workspace_name,\n",
    "                                                subscription_id=subscription_id,\n",
    "                                                resource_group=resource_group,\n",
    "                                                model_name=azure_model,\n",
    "                                                service_name=azure_service,\n",
    "                                                tenant_id=tenant_id,\n",
    "                                                service_principal_id=service_principal_id,\n",
    "                                                service_principal_password=service_principal_password)\n",
    "\n",
    "            azure_deploy_task.after(azure_register_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/8759fa94-806e-4426-9fca-b9fab21848c1\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/142d8329-0bf7-4922-bb43-e84f1f631fe7\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    from parameters import Timestamp\n",
    "    try:\n",
    "        import kfp.compiler as compiler\n",
    "        compiler.Compiler().compile(mxnet_pipeline, 'mxnet_pipeline.tar.gz')\n",
    "    except RuntimeError as err:\n",
    "        logging.debug(err)\n",
    "        logging.info(\"Argo workflow failed validation check but it can still be used to run experiments.\")\n",
    "        \n",
    "    client = None\n",
    "    if execution_mode == \"local\":\n",
    "        client = kfp.Client(host=host)\n",
    "    else:\n",
    "        client = kfp.Client()\n",
    "    blerssi_hybrid_experiment = client.create_experiment(name='BLERSSI-Sagemaker')\n",
    "\n",
    "    run = client.run_pipeline(blerssi_hybrid_experiment.id, 'blerssi-sagemaker-pipeline-'+Timestamp.timestamp, pipeline_package_path='mxnet_pipeline.tar.gz', params={\n",
    "'region': aws_cloud_region,\n",
    "'image': inference_image,\n",
    "'model_name': model_name,\n",
    "'endpoint_config_name': endpoint_config_name,\n",
    "'endpoint_name': endpoint_name,\n",
    "'model_artifact_url': model_path,\n",
    "'instance_type_1': instance_type,\n",
    "'role': role_arn\n",
    "})\n",
    "\n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    from parameters import Timestamp\n",
    "    try:\n",
    "        import kfp.compiler as compiler\n",
    "        compiler.Compiler().compile(gcp_pipeline, 'blerssi_gcp_pipeline.tar.gz')\n",
    "    except RuntimeError as err:\n",
    "        logging.debug(err)\n",
    "        logging.info(\"Argo workflow failed validation check but it can still be used to run experiments.\")\n",
    "        \n",
    "    client = None\n",
    "    if execution_mode == \"local\":\n",
    "        client = kfp.Client(host=host)\n",
    "    else:\n",
    "        client = kfp.Client()\n",
    "    blerssi_hybrid_experiment = client.create_experiment(name='BLERSSI-GCP')\n",
    "        \n",
    "    run = client.run_pipeline(blerssi_hybrid_experiment.id, 'blerssi-gcp-mlengine-pipeline-'+Timestamp.gcp_timestamp, pipeline_package_path='blerssi_gcp_pipeline.tar.gz',\n",
    "                         params={'model_uri': model_path,\n",
    "                                 'project_id': project_id,\n",
    "                                 'model_id': model_name,\n",
    "                                 'version_id': version_name})\n",
    "    \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    #Create a pipeline run\n",
    "    kfp.Client().create_run_from_pipeline_func(azure_pipeline, arguments={})\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check service endpoint status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://95c29701-868a-489c-ba06-d2ae61471110.southcentralus.azurecontainer.io/score\n",
      "2020-06-08T10:21:43,356547238+00:00 - iot-server/run \n",
      "2020-06-08T10:21:43,364039298+00:00 - gunicorn/run \n",
      "2020-06-08T10:21:43,368621896+00:00 - rsyslog/run \n",
      "2020-06-08T10:21:43,371342754+00:00 - nginx/run \n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2020-06-08T10:21:44,207272823+00:00 - iot-server/finish 1 0\n",
      "2020-06-08T10:21:44,224350888+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (11)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 45\n",
      "Initializing logger\n",
      "2020-06-08 10:22:12,208 | root | INFO | Starting up app insights client\n",
      "Starting up app insights client\n",
      "2020-06-08 10:22:12,209 | root | INFO | Starting up request id generator\n",
      "Starting up request id generator\n",
      "2020-06-08 10:22:12,209 | root | INFO | Starting up app insight hooks\n",
      "Starting up app insight hooks\n",
      "2020-06-08 10:22:12,209 | root | INFO | Invoking user's init function\n",
      "Invoking user's init function\n",
      "model path :\n",
      "azureml-models/azuremodel08-06-20-10-13-22/1/blerssi\n",
      "2020-06-08 10:22:12,209 | root | INFO | Users's init has completed successfully\n",
      "Users's init has completed successfully\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2020-06-08 10:22:12,212 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2020-06-08 10:22:12,212 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2020-06-08 10:22:12,213 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "Scoring timeout is found from os.environ: 60000 ms\n",
      "2020-06-08 10:22:12,250 | root | INFO | 200\n",
      "200\n",
      "127.0.0.1 - - [08/Jun/2020:10:22:12 +0000] \"GET /swagger.json HTTP/1.0\" 200 2050 \"-\" \"Go-http-client/1.1\"\n",
      "2020-06-08 10:22:18,105 | root | INFO | 200\n",
      "200\n",
      "127.0.0.1 - - [08/Jun/2020:10:22:18 +0000] \"GET /swagger.json HTTP/1.0\" 200 2050 \"-\" \"Go-http-client/1.1\"\n",
      "2020-06-08 10:23:47,002 | root | INFO | 200\n",
      "200\n",
      "127.0.0.1 - - [08/Jun/2020:10:23:46 +0000] \"GET /swagger.json HTTP/1.0\" 200 2050 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    sg_client = boto3.client('sagemaker', region_name=aws_cloud_region)\n",
    "    resp = sg_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = resp['EndpointStatus']\n",
    "    print(\"Endpoint status:\", endpoint_status)\n",
    "    logging.info(f\"Endpoint status: {endpoint_status}\")\n",
    "    \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}/versions/{}'.format(project_id, model_name, version_name)\n",
    "    \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    svc_pr_password = os.environ.get(\"AZUREML_PASSWORD\")\n",
    "    svc_pr = ServicePrincipalAuthentication(\n",
    "    tenant_id=tenant_id,\n",
    "    service_principal_id=service_principal_id,\n",
    "    service_principal_password=service_principal_password)\n",
    "\n",
    "    ws = Workspace(\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group=resource_group,\n",
    "    workspace_name=workspace_name,\n",
    "    auth=svc_pr\n",
    "    )\n",
    "    \n",
    "    service = Webservice(workspace=ws, name=azure_service)\n",
    "    print(service.scoring_uri)\n",
    "    \n",
    "    print(service.get_logs())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using service endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\": [26]}\n"
     ]
    }
   ],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, content_type= 'application/x-npy', accept= 'application/json')\n",
    "\n",
    "    def _npy_dumps(data):\n",
    "        \"\"\"\n",
    "        Serialized a numpy array into a stream of npy-formatted bytes.\n",
    "        \"\"\"\n",
    "        from six import BytesIO\n",
    "        buffer = BytesIO()\n",
    "        np.save(buffer, data)\n",
    "        return buffer.getvalue()\n",
    "\n",
    "    request_data = _npy_dumps(nd.array([[-200, -200, -200, -75, -200, -200, -200, -200, -200, -200, -200, -200, -200],[-200, -200, -200, -75, -200, -200, -200, -200, -200, -200, -200, -200, -200]]).asnumpy())\n",
    "    result = predictor.predict(data=request_data)\n",
    "\n",
    "    import pickle\n",
    "    depickled_result = pickle.loads(result)\n",
    "\n",
    "    print(\"Outputs, predictions\")\n",
    "    print(depickled_result[0], depickled_result[1])\n",
    "    \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    instances=[{\"b3001\":[1.0] , \"b3002\":[1.0] , \"b3003\":[0.4], \"b3004\":[1.0] , \"b3005\":[0.385] , \"b3006\":[0.280] , \"b3007\":[0.405] , \"b3008\":[1.0] , \"b3009\":[1.0] , \"b3010\":[1.0] , \"b3011\":[1.0] , \"b3012\":[1.0] , \"b3013\":[1.0]}]\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "    else:\n",
    "      print(response['predictions'])\n",
    "\n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    X=[[-200,-200,-200,-200,-200,-63,-200,-200,-200,-200,-200,-200,-200]]\n",
    "    test_samples = json.dumps({\"data\": X})\n",
    "    test_samples = bytes(test_samples, encoding='utf8')\n",
    "    \n",
    "    print(service.run(input_data=test_samples))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up after prediction\n",
    "\n",
    "In case of GCP cloud, please enter service account name & project ID in the placeholders below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: Image class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n"
     ]
    }
   ],
   "source": [
    "if CLOUD_NAME=='aws':\n",
    "    \n",
    "    logging.info(\"Deleting endpoint...\")\n",
    "    predictor.delete_endpoint()\n",
    "    \n",
    "elif CLOUD_NAME=='gcp':\n",
    "    \n",
    "    !gcloud auth activate-service-account <<ACCOUNT>> --key-file=auth.json --project=<<PROJECT ID>>\n",
    "\n",
    "    # Delete version resource\n",
    "    ! gcloud ai-platform versions delete $version_name --quiet --model $model_name \n",
    "\n",
    "    # Delete model resource\n",
    "    ! gcloud ai-platform models delete $model_name --quiet\n",
    "\n",
    "    # Delete Cloud Storage objects that were created\n",
    "    ! gsutil -m rm -r gs://$bucket_name/\n",
    "        \n",
    "elif CLOUD_NAME=='azure':\n",
    "    \n",
    "    #Delete/cleanup created webservice\n",
    "    service.delete()\n",
    "\n",
    "    #Delete created image \n",
    "    image_obj = Image(ws, name=azure_service)\n",
    "    image_obj.delete()\n",
    "\n",
    "    #Delete created model\n",
    "    model_obj = Model(ws, name=azure_model)\n",
    "    model_obj.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
