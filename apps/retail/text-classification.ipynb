{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "This Notebook focuses on implementing multi-class text classification on Amazon automotive reviews dataset by choosing any one combination of various data transformation techniques and algorithms.\n",
    "\n",
    "Rating(1-5) is predicted for each review from the dataset.\n",
    "\n",
    "Best scoring combinations are listed below. Any single combination out of the following can be chosen for data transformation & model training :\n",
    "\n",
    "* Creation of word embeddings using gensim's word2vec & subsequent training using Random Forest algorithm.\n",
    "* Creation of word embeddings using word2vec and/or Smooth Inverse Frequency (SIF) technique & subsequent training using Random Forest algorithm.\n",
    "* Vectorisation using Term frequency-inverse document frequency (Tfidf) technique & subsequent training using Random Forest algorithm.\n",
    "* Vectorisation using Tfidf technique & subsequent training using Linear support vector clustering (SVC) algorithm.\n",
    "\n",
    "| Data Transformation  | Training Algorithm |\n",
    "| ------------- | ------------- |\n",
    "| Word2vec | Random Forest  |\n",
    "| Word2vec + SIF  | Random Forest  |\n",
    "| TfIdf Vectorization  | Random Forest  |\n",
    "| TfIdf Vectorization  | Linear SVC  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.6/site-packages (1.0.5)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied: gensim in ./.local/lib/python3.6/site-packages (3.8.3)\n",
      "Requirement already satisfied: sklearn in ./.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn==0.20.3 in ./.local/lib/python3.6/site-packages (0.20.3)\n",
      "Requirement already satisfied: imbalanced-learn==0.4.3 in ./.local/lib/python3.6/site-packages (0.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.6/site-packages (from nltk) (4.48.0)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in ./.local/lib/python3.6/site-packages (from nltk) (2020.7.14)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.6/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.local/lib/python3.6/site-packages (from gensim) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: boto3 in ./.local/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.14.24)\n",
      "Requirement already satisfied: boto in ./.local/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in ./.local/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./.local/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.24 in ./.local/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in ./.local/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.24->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk gensim sklearn scikit-learn==0.20.3 imbalanced-learn==0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "from joblib import dump\n",
    "import re\n",
    "import nltk as nl\n",
    "import gensim\n",
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#nltk\n",
    "from nltk.corpus import stopwords\n",
    "nl.download('punkt')\n",
    "nl.download('stopwords')\n",
    "\n",
    "#Over-sampling\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dataset from JSON format to CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = pd.read_json('data/amazon_automotive_reviews.json', lines=True)\n",
    "json_data.to_csv('amazon_automotive_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('amazon_automotive_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3F73SC1LY51OO</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>Alan Montgomery</td>\n",
       "      <td>[4, 4]</td>\n",
       "      <td>I needed a set of jumper cables for my new car...</td>\n",
       "      <td>5</td>\n",
       "      <td>Work Well - Should Have Bought Longer Ones</td>\n",
       "      <td>1313539200</td>\n",
       "      <td>08 17, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A20S66SKYXULG2</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>alphonse</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>These long cables work fine for my truck, but ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Okay long cables</td>\n",
       "      <td>1315094400</td>\n",
       "      <td>09 4, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2I8LFSN2IS5EO</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>Chris</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Can't comment much on these since they have no...</td>\n",
       "      <td>5</td>\n",
       "      <td>Looks and feels heavy Duty</td>\n",
       "      <td>1374710400</td>\n",
       "      <td>07 25, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A3GT2EWQSO45ZG</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>DeusEx</td>\n",
       "      <td>[19, 19]</td>\n",
       "      <td>I absolutley love Amazon!!!  For the price of ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent choice for Jumper Cables!!!</td>\n",
       "      <td>1292889600</td>\n",
       "      <td>12 21, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3ESWJPAVRPWB4</td>\n",
       "      <td>B00002243X</td>\n",
       "      <td>E. Hernandez</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I purchased the 12' feet long cable set and th...</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent, High Quality Starter Cables</td>\n",
       "      <td>1341360000</td>\n",
       "      <td>07 4, 2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin     reviewerName   helpful  \\\n",
       "0  A3F73SC1LY51OO  B00002243X  Alan Montgomery    [4, 4]   \n",
       "1  A20S66SKYXULG2  B00002243X         alphonse    [1, 1]   \n",
       "2  A2I8LFSN2IS5EO  B00002243X            Chris    [0, 0]   \n",
       "3  A3GT2EWQSO45ZG  B00002243X           DeusEx  [19, 19]   \n",
       "4  A3ESWJPAVRPWB4  B00002243X     E. Hernandez    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I needed a set of jumper cables for my new car...        5   \n",
       "1  These long cables work fine for my truck, but ...        4   \n",
       "2  Can't comment much on these since they have no...        5   \n",
       "3  I absolutley love Amazon!!!  For the price of ...        5   \n",
       "4  I purchased the 12' feet long cable set and th...        5   \n",
       "\n",
       "                                      summary  unixReviewTime   reviewTime  \n",
       "0  Work Well - Should Have Bought Longer Ones      1313539200  08 17, 2011  \n",
       "1                            Okay long cables      1315094400   09 4, 2011  \n",
       "2                  Looks and feels heavy Duty      1374710400  07 25, 2013  \n",
       "3       Excellent choice for Jumper Cables!!!      1292889600  12 21, 2010  \n",
       "4      Excellent, High Quality Starter Cables      1341360000   07 4, 2012  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['overallRating'] = raw_data['overall']\n",
    "raw_data = raw_data.drop(['reviewerID','asin','reviewerName','helpful','overall','summary','unixReviewTime','reviewTime'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean review text column and remove punctuations and numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['p_review'] = raw_data['reviewText'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','', str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose data transformation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation can be done using either word2vec or Smooth inverse frequency (sif) technique or Tf-Idf vectorization (tfidf).\n",
    "# Choose from ==> ['word2vec', 'sif', 'tfidf']\n",
    "\n",
    "data_transform = 'sif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose model training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training can be done using either random forest (rf) or Linear Support vector clustering (lsvc) algorithms.\n",
    "#Choose from ==> ['rf','lsvc']\n",
    "\n",
    "train_algorithm = 'rf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate data transformation method & training algorithm options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_transform or data_transform not in ['word2vec', 'sif', 'tfidf']:\n",
    "     raise ValueError(\"Set a valid method to perform data transformation (word2vec/sif/tfidf)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not train_algorithm or train_algorithm not in ['rf','lsvc']:\n",
    "     raise ValueError(\"Set a valid algorithm to train your model(rf/lsvc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_transform in ['word2vec','sif'] and train_algorithm == 'lsvc':\n",
    "    raise Warning(\"The combination selected may not be the best scoring one!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply data transformation on data as per selected choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec model generated & trained on tokens from review text\n"
     ]
    }
   ],
   "source": [
    "if data_transform in ['word2vec', 'sif']:\n",
    "\n",
    "        p_review = raw_data['p_review'].to_list()\n",
    "\n",
    "        tokens = [nl.word_tokenize(sentences) for sentences in p_review]\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "        tokens = [[word for word in tokens[i] if not word in stopwords.words('english')] for i in range(len(tokens))]\n",
    "\n",
    "        wv_model = gensim.models.Word2Vec(tokens, size=300, min_count=1, workers=4)\n",
    "\n",
    "        wv_model.train(tokens, total_examples=len(tokens), epochs=50)\n",
    "        \n",
    "        print(\"Word2vec model generated & trained on tokens from review text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data using Smooth inverse frequency(SIF)..\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "if data_transform == 'word2vec':\n",
    "        \n",
    "        print(\"Preparing training data using word2vec..\")\n",
    "        wv_train = []\n",
    "        for i in range(len(tokens)):\n",
    "            wv_train.append(np.mean(np.asarray([wv_model[token] for token in tokens[i]]),axis=0))\n",
    "        print(\"Completed\")\n",
    "            \n",
    "elif data_transform == 'sif':\n",
    "    \n",
    "        print(\"Preparing training data using Smooth inverse frequency(SIF)..\")\n",
    "        vlookup = wv_model.wv.vocab\n",
    "        Z = 0\n",
    "        for k in vlookup:\n",
    "                Z += vlookup[k].count # Compute the normalization constant Z\n",
    "\n",
    "        a = 0.001\n",
    "        embedding_size = 300\n",
    "        wv_sif_train = []\n",
    "        for i in range(len(tokens)):\n",
    "                vs = np.zeros(300)\n",
    "                for word in tokens[i]:\n",
    "                        a_value = a / (a + (vlookup[word].count/Z))\n",
    "                        vs = np.add(vs, np.multiply(a_value, wv_model.wv[word]))\n",
    "                wv_sif_train.append(np.divide(vs, len(tokens[i])))\n",
    "        print(\"Completed\")\n",
    "                \n",
    "elif data_transform == 'tfidf':\n",
    "         print(\"Preparing training data using TfIdf vectorization..\")\n",
    "         tfidf = TfidfVectorizer(ngram_range=(1,2),sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', stop_words='english')\n",
    "         features = tfidf.fit_transform(raw_data.p_review).toarray()\n",
    "         print(features.shape)\n",
    "         print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depict class imbalance issue in dataset using value count for each rating\n",
    "\n",
    "Here rating 5 has lot more records than others, so the dataset is considered to be highly skewed / imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    13928\n",
       "4     3967\n",
       "3     1430\n",
       "2      606\n",
       "1      542\n",
       "Name: overallRating, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.overallRating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize target variable to a local variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = raw_data.overallRating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample dataset to remove class imbalance issue using SMOTENC\n",
    "\n",
    "Preprocessing of the dataset is done in such a way that the rating categories other than 5 ( which is the majority class) is oversampled accordingly, so as to get a balanced dataset without any prediction output bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset samples per class Counter({5: 13928, 4: 11000, 3: 6800, 2: 6200, 1: 6000})\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTENC(sampling_strategy={1: 6000, 2: 6200, 3 : 6800, 4: 11000}, random_state=42, categorical_features=[1])\n",
    "if data_transform == 'word2vec':\n",
    "    X_resampled, y_resampled = sm.fit_resample(np.asarray(wv_train), y)\n",
    "    \n",
    "elif data_transform == 'sif':\n",
    "    X_resampled, y_resampled = sm.fit_resample(np.asarray(wv_sif_train), y)\n",
    "    \n",
    "elif data_transform == 'tfidf':\n",
    "    X_resampled, y_resampled = sm.fit_resample(features, y)\n",
    "\n",
    "print('Resampled dataset samples per class {}'.format(Counter(y_resampled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_transform == 'word2vec':\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.3,shuffle=True,random_state=7)\n",
    "\n",
    "elif data_transform == 'sif':\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.3,shuffle=True,random_state=7)\n",
    "\n",
    "elif data_transform == 'tfidf':\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_resampled,y_resampled,test_size=0.3,shuffle=True,random_state=7)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_algorithm == 'rf':\n",
    "    model = RandomForestClassifier(n_estimators=40, random_state=0)\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "elif train_algorithm == 'lsvc':\n",
    "    model = LinearSVC()\n",
    "    model.fit(x_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/model.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rel_path = 'model/'\n",
    "file_name = 'model.joblib'\n",
    "\n",
    "if not os.path.exists(file_rel_path):\n",
    "    os.mkdir(file_rel_path)\n",
    "dump(model, file_rel_path + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define inference service name & model storage URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pvc://workspace-poornima/model/\n"
     ]
    }
   ],
   "source": [
    "svc_name = 'text-classify'\n",
    "\n",
    "!kubectl get pods $HOSTNAME -o yaml -n anonymous > podspec\n",
    "with open(\"podspec\") as f:\n",
    "    content = yaml.safe_load(f)\n",
    "    for elm in content['spec']['volumes']:\n",
    "        if 'workspace-' in elm['name']:\n",
    "            pvc = elm['name']\n",
    "os.remove('podspec')\n",
    "pvc\n",
    "    \n",
    "storageURI = \"pvc://\" + pvc + '/' + file_rel_path\n",
    "print(storageURI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define configuration for inference service creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apiVersion: serving.kubeflow.org/v1alpha2\r\n",
      "kind: InferenceService\r\n",
      "metadata:\r\n",
      "  name: text-classify\r\n",
      "  namespace: anonymous\r\n",
      "spec:\r\n",
      "  default:\r\n",
      "    predictor:\r\n",
      "      sklearn:\r\n",
      "        storageUri: pvc://workspace-poornima/model/\r\n"
     ]
    }
   ],
   "source": [
    "wsvol_blerssi_kf = f\"\"\"apiVersion: \"serving.kubeflow.org/v1alpha2\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: {svc_name}\n",
    "  namespace: anonymous\n",
    "spec:\n",
    "  default:\n",
    "    predictor:\n",
    "      sklearn:\n",
    "        storageUri: {storageURI}\n",
    "\"\"\"\n",
    "    \n",
    "kfserving = yaml.safe_load(wsvol_blerssi_kf)\n",
    "with open('blerssi-kfserving.yaml', 'w') as file:\n",
    "    yaml_kfserving = yaml.dump(kfserving,file)\n",
    "\n",
    "! cat blerssi-kfserving.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the configuration .yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kubeflow.org/text-classify created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f blerssi-kfserving.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether inferenceservice is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            URL                                                                  READY   DEFAULT TRAFFIC   CANARY TRAFFIC   AGE\r\n",
      "text-classify   http://text-classify.anonymous.example.com/v1/models/text-classify   True    100                                105s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get inferenceservice -n anonymous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "Wait for inference service READY=\"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict data from serving after setting INGRESS_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_name = svc_name + '.anonymous.example.com'\n",
    "\n",
    "headers = { \n",
    "    'host': host_name\n",
    "}\n",
    "\n",
    "formData = {\n",
    "    'instances': x_test[:1].tolist()\n",
    "}\n",
    "url = 'http://<<INGRESS_IP>>:31380/v1/models/' + svc_name + ':predict'\n",
    "res = requests.post(url, json=formData, headers=headers)\n",
    "results = res.json()\n",
    "prediction = results['predictions']\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up after predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete inference service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kubeflow.org \"text-classify\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f blerssi-kfserving.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $file_rel_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
