{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Pipeline on UCS using Darknet & YOLO\n",
    "\n",
    "This notebook focuses on implementing object detection as a Kubeflow pipeline on Cisco UCS by using Darknet which is a open-source neural network framework, YOLO (You Only Look Once) which is a real-time object detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Cisco Kubeflow starter pack repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_NAME=\"dev\" #Provide git branch \"master\" or \"dev\"\n",
    "! git clone -b $BRANCH_NAME https://github.com/CiscoAI/cisco-kubeflow-starter-pack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp==1.0.1 pillow==7.2.0 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import calendar\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "#Kubeflow\n",
    "import kfp\n",
    "from kfp.aws import use_aws_secret\n",
    "import kfp.compiler as compiler\n",
    "\n",
    "#Kubernetes\n",
    "from kubernetes import client\n",
    "\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pipeline components\n",
    "\n",
    "Declare the paths of respective YAML configuration files of each of the pipeline components, in order to load each component into a variable for pipeline execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='cisco-kubeflow-starter-pack/apps/computer-vision/object-detection/onprem/pipeline/components/v2/'\n",
    "component_root_dwn= path+'download/'\n",
    "component_root_katib= path+'katib/'\n",
    "component_root_train= path+'train/'\n",
    "component_root_validate= path+'validate/'\n",
    "component_root_convert=path+'conversion/'\n",
    "component_root_kfserving=path+'kfserving/'\n",
    "\n",
    "download_op = kfp.components.load_component_from_file(os.path.join(component_root_dwn, 'component.yaml'))\n",
    "hptuning_op = kfp.components.load_component_from_file(os.path.join(component_root_katib, 'component.yaml'))\n",
    "train_op = kfp.components.load_component_from_file(os.path.join(component_root_train, 'component.yaml'))\n",
    "validate_op = kfp.components.load_component_from_file(os.path.join(component_root_validate, 'component.yaml'))\n",
    "convert_op=kfp.components.load_component_from_file(os.path.join(component_root_convert, 'component.yaml'))\n",
    "kfserving_op = kfp.components.load_component_from_file(os.path.join(component_root_kfserving, 'component.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define volume claim & volume mount for storage during pipeline execution\n",
    "\n",
    "Persistent volume claim & volume mount is created for the purpose of storing entities such as Dataset, model files, etc, and to share the stored resources between the various components of the pipeline during it's execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_pvc = client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs')\n",
    "nfs_volume = client.V1Volume(name='nfs', persistent_volume_claim=nfs_pvc)\n",
    "nfs_volume_mount = client.V1VolumeMount(mount_path='/mnt/', name='nfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus=4 # Number of GPUs to run training\n",
    "\n",
    "def object_detection_pipeline(\n",
    "    nfs_path='/mnt/object_detection',\n",
    "    s3_path=\"s3://object-det-test/001\",    # AWS S3 bucket URL. Ex: s3://<bucket-name>/ \n",
    "    namespace='kubeflow',               # Namespace on which trained model is to be deployed for prediction\n",
    "    inferenceservice_name=\"\",           # Name of inference service and model name \n",
    "    timestamp=\"\",                       # Current timestamp\n",
    "    cfg_data=\"voc.data\",                # Config file containing file name specifications of train, test and validate datasets\n",
    "    cfg_file=\"yolov3-voc.cfg\",          # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "    weights=\"yolov3-voc_50000.weights\", # Weights which are already pre-trained upto 50000 iterations is used. Therefore,  \n",
    "                                        # training happens from 50000 iterations upto a limit of max_batches (say 50200) specified \n",
    "                                        # in cfg_file. \n",
    "    trials=2,                           # Total number of trials under Katib experiment\n",
    "    gpus_per_trial=1,                   # Maximum GPUS to be used for each trial\n",
    "    classes_file=\"voc.names\"            # File containing the names of object classes (such as person, bus, car,etc)\n",
    "):\n",
    "#     # Download component\n",
    "    dwn_task = download_op(s3_path=s3_path,timestamp=timestamp,\n",
    "                           cfg_data=cfg_data\n",
    "                          ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    dwn_task.add_volume(nfs_volume)\n",
    "    dwn_task.add_volume_mount(nfs_volume_mount) \n",
    "    \n",
    "#     # HP tuning (Katib) component\n",
    "    hptuning_task = hptuning_op(cfg_data=cfg_data,             # Config file containing file name specifications of train, test and validate datasets\n",
    "                                cfg_file=cfg_file,             # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                                weights=weights,               # Pre-trained weights for VOC dataset\n",
    "                                trials=trials,                 # total number of trials under Katib experiment\n",
    "                                timestamp=timestamp,           # Current timestamp to create unique experiment \n",
    "                                                               # Ex: object-detection-1599547688-random-588d7877f5-zvlx5\n",
    "                                gpus_per_trial=gpus_per_trial, # Maximum GPUS to be used for each trial \n",
    "                                )\n",
    "    hptuning_task.add_volume(nfs_volume)\n",
    "    hptuning_task.add_volume_mount(nfs_volume_mount)\n",
    "    hptuning_task.after(dwn_task)\n",
    "    \n",
    "    # Train component\n",
    "    train_task = train_op(cfg_data=cfg_data,          # Config file containing file name specifications of train, test and validate datasets\n",
    "                          cfg_file=cfg_file,          # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                          weights=weights,            # Pre-trained weights for VOC dataset\n",
    "                          gpus=gpus,             \n",
    "                          timestamp=timestamp\n",
    "                         )\n",
    "    train_task.add_volume(nfs_volume)\n",
    "    train_task.add_volume_mount(nfs_volume_mount).set_gpu_limit(gpus)  #Maximum GPUs to be used for training\n",
    "    train_task.after(hptuning_task)\n",
    "    \n",
    "    # Validation component\n",
    "    validate_task = validate_op(nfs_path=nfs_path,\n",
    "                                s3_path=s3_path,\n",
    "                                cfg_data=cfg_data,          # Config file containing file name specifications of train, test and validate datasets\n",
    "                                cfg_file=cfg_file,          # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                                weights=weights,            # Pre-trained weights for VOC dataset\n",
    "                                timestamp=timestamp\n",
    "                                ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    validate_task.add_volume(nfs_volume)\n",
    "    validate_task.add_volume_mount(nfs_volume_mount)\n",
    "    validate_task.after(train_task)\n",
    "    \n",
    "    # Model conversion component\n",
    "    conversion_task=convert_op(push_to_s3=\"true\",  # Flag to decide whether to upload the trained weights and converted\n",
    "                                                    # model to S3 bucket for future inferencing on anyother environment or\n",
    "                                                    # proceeding to serve on UCS (Input: true/false)\n",
    "                               s3_path=s3_path,timestamp=timestamp,\n",
    "                               classes_file=classes_file\n",
    "                               ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    conversion_task.add_volume(nfs_volume)\n",
    "    conversion_task.add_volume_mount(nfs_volume_mount)\n",
    "    conversion_task.after(validate_task)\n",
    "    \n",
    "    # KFserving component\n",
    "    kfserving = kfserving_op(inferenceservice_name=inferenceservice_name,\n",
    "                            classes_file=classes_file,timestamp=timestamp,\n",
    "                            namespace=namespace).set_image_pull_policy('Always')\n",
    "    kfserving.add_volume(nfs_volume)\n",
    "    kfserving.add_volume_mount(nfs_volume_mount)\n",
    "    kfserving.after(conversion_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile pipeline function\n",
    "\n",
    "Compile the pipeline function to create a tar ball for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"2\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"1\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "# Compile pipeline\n",
    "try:\n",
    "    compiler.Compiler().compile(object_detection_pipeline, 'object-detection.tar.gz')\n",
    "except RuntimeError as err:\n",
    "    logging.debug(err)\n",
    "    logging.info(\"Argo workflow failed validation check but it can still be used to run experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/c3aa019c-9bb5-4f88-a64a-33996d77f2e1\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kp_client = kfp.Client()\n",
    "EXPERIMENT_NAME = 'Object Detection'\n",
    "experiment = kp_client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize pipeline parameters & run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/1f667a28-3b6a-4010-b682-50c05ddf2447\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pipeline parameters\n",
    "timestamp = str(calendar.timegm(time.gmtime()))\n",
    "trials=1\n",
    "gpus_per_trial=2\n",
    "inferenceservice_name=\"object-detection-%s\"%timestamp\n",
    "\n",
    "# Execute pipeline\n",
    "run = kp_client.run_pipeline(experiment.id, 'object-detection', 'object-detection.tar.gz', \n",
    "                          params={\"timestamp\": timestamp,\n",
    "                                  \"inferenceservice_name\": inferenceservice_name,\n",
    "                                  \"trials\": trials,\n",
    "                                  \"gpus_per_trial\": gpus_per_trial})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a prediction\n",
    "### Check if inference service is Ready\n",
    "Before running a prediction, make sure that pipeline run is completed in the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get inferenceservice $inferenceservice_name -n kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Wait for inference service READY=\"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER='ucs' #where your cluster is running :'gcp' or 'ucs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get cluster & inference details\n",
    "\n",
    "Get Ingress IP, Port and Inferenceservice host name to be used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$CLUSTER\" \"$inferenceservice_name\" --out NODE_INFERENCE\n",
    "if [ $1 = \"ucs\" ]\n",
    "then\n",
    "    echo \"$(kubectl get node -o=jsonpath='{.items[0].status.addresses[0].address}')\"\n",
    "else\n",
    "    echo \"$(kubectl get node -o=jsonpath='{.items[0].status.addresses[1].address}')\"\n",
    "fi\n",
    "\n",
    "INGRESS_GATEWAY=\"istio-ingressgateway\"\n",
    "echo \"$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.spec.ports[1].nodePort}')\"\n",
    "\n",
    "echo \"$(kubectl get inferenceservice $2 -n kubeflow -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Pre-processing, Prediction & Post-processing\n",
    "\n",
    "* Define functions for preprocessing Image & predicting object detection boxes.\n",
    "* Predicted output is post-processed to display object detection boxes of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Client data preprocess function\n",
    "def letter_box_image(image: Image.Image, output_height: int, output_width: int, fill_value)-> np.ndarray:\n",
    "    height_ratio = float(output_height)/image.size[1]\n",
    "    width_ratio = float(output_width)/image.size[0]\n",
    "    fit_ratio = min(width_ratio, height_ratio)\n",
    "    fit_height = int(image.size[1] * fit_ratio)\n",
    "    fit_width = int(image.size[0] * fit_ratio)\n",
    "    fit_image = np.asarray(image.resize((fit_width, fit_height), resample=Image.BILINEAR))\n",
    "\n",
    "    if isinstance(fill_value, int):\n",
    "        fill_value = np.full(fit_image.shape[2], fill_value, fit_image.dtype)\n",
    "\n",
    "    to_return = np.tile(fill_value, (output_height, output_width, 1))\n",
    "    pad_top = int(0.5 * (output_height - fit_height))\n",
    "    pad_left = int(0.5 * (output_width - fit_width))\n",
    "    to_return[pad_top:pad_top+fit_height, pad_left:pad_left+fit_width] = fit_image\n",
    "    return to_return\n",
    "\n",
    "# Function to predict object detected image data\n",
    "def predict(img_data):\n",
    "    headers = {\"Host\": NODE_INFERENCE.split('\\n')[2]}\n",
    "    data={\"instances\":img_data.tolist()}\n",
    "    url = \"http://%s:%s/v1/models/%s:predict\"%(NODE_INFERENCE.split(\"\\n\")[0],NODE_INFERENCE.split(\"\\n\")[1], inferenceservice_name)\n",
    "    response=requests.post(url, data=json.dumps(data), headers=headers)\n",
    "    boxes=response.json()['predictions'][0]\n",
    "    classes=response.json()['predictions'][1]\n",
    "    scores=response.json()['predictions'][2]\n",
    "    return boxes, classes, scores\n",
    "\n",
    "#Post-process predicted output\n",
    "def letter_box_pos_to_original_pos(letter_pos, current_size, ori_image_size)-> np.ndarray:\n",
    "    letter_pos = np.asarray(letter_pos, dtype=np.float)\n",
    "    current_size = np.asarray(current_size, dtype=np.float)\n",
    "    ori_image_size = np.asarray(ori_image_size, dtype=np.float)\n",
    "    final_ratio = min(current_size[0]/ori_image_size[0], current_size[1]/ori_image_size[1])\n",
    "    pad = 0.5 * (current_size - final_ratio * ori_image_size)\n",
    "    pad = pad.astype(np.int32)\n",
    "    to_return_pos = (letter_pos - pad) / final_ratio\n",
    "    return to_return_pos\n",
    "\n",
    "def convert_to_original_size(box, size, original_size, is_letter_box_image):\n",
    "    if is_letter_box_image:\n",
    "        box = box.reshape(2, 2)\n",
    "        box[0, :] = letter_box_pos_to_original_pos(box[0, :], size, original_size)\n",
    "        box[1, :] = letter_box_pos_to_original_pos(box[1, :], size, original_size)\n",
    "    else:\n",
    "        ratio = original_size / size\n",
    "        box = box.reshape(2, 2) * ratio\n",
    "    return list(box.reshape(-1))\n",
    "\n",
    "def draw_boxes(boxes, classes, scores, img, detection_size, is_letter_box_image):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    color = 255,0,0#np.random.randint(0, 256, 3))\n",
    "    for box, score, cls in zip(boxes, scores, classes):\n",
    "        box = convert_to_original_size(box, np.array(detection_size),\n",
    "                                       np.array(img.size),\n",
    "                                       is_letter_box_image)\n",
    "        draw.rectangle(box, outline=color)\n",
    "        draw.text(box[:2], '{} {:.2f}%'.format(cls, score * 100), fill=color)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call client data preprocess function\n",
    "img = Image.open('dog.jpg')\n",
    "img_resized = letter_box_image(img, 416, 416, 128)\n",
    "img_resized = img_resized.astype(np.float32)\n",
    "\n",
    "#Call predict function\n",
    "boxes, classes, scores = predict(img_resized)\n",
    "draw_boxes(np.asarray(boxes), classes, np.asarray(scores), img, (416, 416), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Clean up deployed inference service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete inferenceservice $inferenceservice_name -n kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
